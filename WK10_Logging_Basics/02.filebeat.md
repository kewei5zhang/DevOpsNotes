# Integrate the ELK with filebeat

![Alt text](images/high_level.png?raw=true)
reference: https://www.freecodecamp.org/news/docker-container-log-analysis-with-elastic-stack-53d5ec9e5953/

## Step 1. Let us update logstash and filebeat configurations 
Go to the docker-elk folder that you cloned (mentioned in 01.elk_hands_on.md)
```
cd docker-elk
```
Open logstash -> pipeline -> logstash.conf and replace the index from `logstash-%{+YYYY.MM.dd}` to 
```
index => "%{[@metadata][beat]}-%{[@metadata][version]}"
```
on line 18 
``` json
... 
output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "logstash_internal"
		password => "${LOGSTASH_INTERNAL_PASSWORD}"
		index => "%{[@metadata][beat]}-%{[@metadata][version]}"
	}
}
```
``` json
# This section defines the output plugins that Logstash will use to send processed data. Outputs are the final phase in the Logstash pipeline, where the data is sent to a specified destination. Multiple outputs can be defined.
output { 
	elasticsearch {
		hosts => "elasticsearch:9200" # Logstash is directed to send data to the Elasticsearch service running on the host named elasticsearch at port 9200. 
		user => "logstash_internal"
		password => "${LOGSTASH_INTERNAL_PASSWORD}"
		index => "%{[@metadata][beat]}-%{[@metadata][version]}" 
        # This setting configures the index name pattern for the data being stored in Elasticsearch. It uses fields from the event's metadata to dynamically set the index name. Here, %{[@metadata][beat]} is replaced by the value of the beat metadata field, and %{[@metadata][version]} is replaced by the version number contained in the event's metadata. This allows for the data to be organized into indices based on the source (beat) and version, facilitating easier data management and querying within Elasticsearch.
	}
}
```
This is to index the logs that are sent from filebeats

Update filebeat.yml under extensions/filebeat/config/filebeat.yml to point filebeat output to logstash

```
monitoring:
  enabled: true
  elasticsearch:
    hosts: ["http://elasticsearch:9200"]
    username: "beats_system"
    password: "${BEATS_SYSTEM_PASSWORD}"

output.logstash:
  hosts: [ logstash:5044 ]

# output.elasticsearch:
#   hosts: [ http://elasticsearch:9200 ]
#   username: filebeat_internal
#   password: ${FILEBEAT_INTERNAL_PASSWORD}  
```

## Step 2. Let us build the filebeat docker container
```
cd extensions/filebeat
docker build -t fcc/filebeat . --build-arg ELASTIC_VERSION=8.12.2
```
You can find the version in .env or http://localhost:9200/
Build the docker image

## Step 3. Run the filebeat agent to collect docker logs
Run filebeat docker compoase along with elastic search, kibana and logstash

```
docker compose down

docker compose --env-file .env -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml up 
```

<!-- ```
docker run --rm -v '/var/lib/docker/containers:/usr/share/dockerlogs/data:ro' -v '/var/run/docker.sock:/var/run/docker.sock' --name filebeat fcc/filebeat:latest
```

In the above Docker command, note the two bind mount parameters: /var/lib/docker/containers is the path where docker 
logs exist within the host machine, and it has been bound to /usr/share/dockerlogs/data path within Filebeat container
with read only access.

In the second bind mount argument, /var/run/docker.sock is bound into the Filebeat containerâ€™s Docker daemon. 
It is the unix socket the Docker daemon listens on by default and it can be used to communicate with the daemon 
from within a container. This allows our Filebeat container to obtain Docker metadata and enrich the container log 
entries along with the metadata and push it to ELK stack. -->

## Step 5. login to kibana and try to create the index (Fixed)
Login to Kibana and Go to `Stack Management` -> `Kibana` -> `Data View` -> `Create`

![Alt text](images/create_data_view.png?raw=true)

You will not see `filebeat-7.2.0` index patterns but some generic ones only.

This is because we have errors in our logs

```
2024-04-03 15:39:02 [2024-04-03T04:39:02,801][INFO ][logstash.outputs.elasticsearch][main][7d69072b556b9857ecc94312fcb7f670c1a168f357ca707fc46d6a48832ca80b] Retrying failed action {:status=>403, :action=>["create", {:_id=>nil, :_index=>"filebeat-8.12.2", :routing=>nil}, {"docker"=>{"container"=>{"labels"=>{"org_label-schema_build-date"=>"2024-02-19T12:06:34.117Z", "org_label-schema_url"=>"https://www.elastic.co/products/kibana", "org_opencontainers_image_vendor"=>"Elastic", "org_opencontainers_image_title"=>"Kibana", "org_label-schema_version"=>"8.12.2", "org_opencontainers_image_version"=>"8.12.2", "com_docker_compose_oneoff"=>"False", "desktop_docker_io/binds/0/Target"=>"/usr/share/kibana/config/kibana.yml", "org_label-schema_schema-version"=>"1.0", "com_docker_compose_project_config_files"=>"/Users/keweizhang/git/docker-elk/docker-compose.yml,/Users/keweizhang/git/docker-elk/extensions/filebeat/filebeat-compose.yml", "com_docker_compose_project"=>"docker-elk", "com_docker_compose_service"=>"kibana", "org_label-schema_vcs-url"=>"https://github.com/elastic/kibana", "com_docker_compose_version"=>"2.23.3", "org_opencontainers_image_created"=>"2024-02-19T12:06:34.117Z", "org_label-schema_license"=>"Elastic License", "org_label-schema_usage"=>"https://www.elastic.co/guide/en/kibana/reference/index.html", "org_opencontainers_image_url"=>"https://www.elastic.co/products/kibana", "com_docker_compose_project_working_dir"=>"/Users/keweizhang/git/docker-elk", "desktop_docker_io/binds/0/SourceKind"=>"hostFile", "org_label-schema_vendor"=>"Elastic", "com_docker_compose_container-number"=>"1", "org_opencontainers_image_revision"=>"f5bd489c5ff9c676c4f861c42da6ea99ae350832", "org_opencontainers_image_licenses"=>"Elastic License", "org_opencontainers_image_source"=>"https://github.com/elastic/kibana", "org_opencontainers_image_ref_name"=>"ubuntu", "org_label-schema_name"=>"Kibana", "org_opencontainers_image_documentation"=>"https://www.elastic.co/guide/en/kibana/reference/index.html", "com_docker_compose_image"=>"sha256:1feabd012adb9592d48cadcf7ca5e402e2dee88aefad00b74b9a1941699b8510", "com_docker_compose_project_environment_file"=>"/Users/keweizhang/git/docker-elk/.env", "desktop_docker_io/binds/0/Source"=>"/Users/keweizhang/git/docker-elk/kibana/config/kibana.yml", "com_docker_compose_depends_on"=>"elasticsearch:service_started:false", "com_docker_compose_config-hash"=>"98f787327b05b68ed4abcce98c53ad0aafa68145698c95bfd84c2da2035ee9c6", "org_label-schema_vcs-ref"=>"f5bd489c5ff9c676c4f861c42da6ea99ae350832"}}}, "stream"=>"stdout", "container"=>{"id"=>"701505ba1babde1a0433cf2c1a1aa1208d7969d6f8e0bbe2ff1f02e2fc81c40d", "image"=>{"name"=>"docker-elk-kibana"}, "name"=>"docker-elk-kibana-1"}, "message"=>"[2024-04-03T04:36:49.863+00:00][INFO ][savedobjects-service] [.kibana_analytics] Migration completed after 293ms", "log"=>{"offset"=>24735, "file"=>{"path"=>"/var/lib/docker/containers/701505ba1babde1a0433cf2c1a1aa1208d7969d6f8e0bbe2ff1f02e2fc81c40d/701505ba1babde1a0433cf2c1a1aa1208d7969d6f8e0bbe2ff1f02e2fc81c40d-json.log"}}, "host"=>{"name"=>"filebeat"}, "@timestamp"=>2024-04-03T04:36:49.863Z, "agent"=>{"ephemeral_id"=>"2aed7c98-d9fb-4bb0-9cf4-2cececa87cb2", "id"=>"af1e9abd-7721-48b5-b68b-3c4e03e6d0c7", "type"=>"filebeat", "version"=>"8.12.2", "name"=>"filebeat"}, "ecs"=>{"version"=>"8.0.0"}, "input"=>{"type"=>"container"}, "event"=>{"original"=>"[2024-04-03T04:36:49.863+00:00][INFO ][savedobjects-service] [.kibana_analytics] Migration completed after 293ms"}, "@version"=>"1", "tags"=>["beats_input_codec_plain_applied"]}], :error=>{"type"=>"security_exception", "reason"=>"action [indices:data/write/bulk[s]] is unauthorized for user [logstash_internal] with effective roles [logstash_writer] on indices [filebeat-8.12.2], this action is granted by the index privileges [create_doc,create,delete,index,write,all]"}}
```
Search `unauthorized for user [logstash_internal] with effective roles [logstash_writer]`

You will find this thread https://github.com/deviantony/docker-elk/issues/687

## Step 6. Add required permission to the logstash_internal role

Go to Stach Management -> Roles and select the `logstash-writer` role, add `filebeat-*` to index priviledges to allow logstash_write role to manage the filebeat-* index

For simplicity, some may take the option to give logstash_internal a superuser privilege. (Not recommended)

![Alt text](images/logstash-writer-role.png?raw=true)
![Alt text](images/superuser.png?raw=true)
<!-- 
After updating, you should see no more errors. However, you should never do this in production.

Instead, search the error with ChatGPT, you will get a more acceptable answer

The error message you provided appears to be related to a security exception in Elasticsearch. Let me break down the error message for you:

1. `type`: This field indicates the type of exception that occurred, which is a "security_exception" in this case. Security exceptions typically occur when there are issues related to user permissions and access control.

2. `reason`: The "reason" field provides a description of why the security exception occurred. In this case, it states that the action `[indices:admin/auto_create]` is unauthorized for the user `[logstash_internal]` with effective roles `[logstash_writer]` on the indices `[filebeat-7.2.0]`. It also mentions that the required action is granted by the index privileges `[auto_configure, create_index, manage, all]`.

To resolve this issue, you should consider the following steps:

1. **Review User and Role Permissions**: Ensure that the user `[logstash_internal]` has the appropriate permissions and roles assigned. In this case, it appears that the user has the `logstash_writer` role, but it may not have the necessary index privileges to perform the action `[indices:admin/auto_create]`.

2. **Grant Necessary Index Privileges**: As mentioned in the error message, the required action is granted by the index privileges `[auto_configure, create_index, manage, all]`. You may need to modify the user's role to include these index privileges or create a new role with the required privileges and assign it to the user.

3. **Check Index Settings**: Verify the settings of the index `[filebeat-7.2.0]`. Make sure that it allows the necessary actions, including index creation, according to the privileges defined in the user's role.

4. **Review Configuration**: Check your Logstash configuration to ensure that it is using the correct user and role for interacting with Elasticsearch. Any misconfiguration in Logstash can lead to authorization issues.

5. **Elasticsearch Version Compatibility**: Ensure that the version of Elasticsearch, Logstash, and the Beats (e.g., Filebeat) components are compatible and have no known security-related issues. Sometimes, upgrading or patching components can resolve such issues.

6. **Logging and Monitoring**: Implement proper logging and monitoring to help diagnose and troubleshoot security-related issues in your Elasticsearch cluster. This can provide insights into the root cause of the problem.

7. **Consult Elasticsearch Documentation**: If you continue to face issues, consult the official Elasticsearch documentation or seek assistance from your organization's Elasticsearch administrators or support team.

By addressing these points and ensuring that the user has the necessary permissions and index privileges, you should be able to resolve the security exception in Elasticsearch. -->

<!-- ---

All you need to do is to go to roles -> logstash_writer -> index privileges add filebeat-* in the indices -->

If you see the below errors:
```
Error response from daemon: network 855e3ee4e282c9d96bb118f6a815f0893843859ae105a9d99e391dbf64028a87 not found
```
Run `docker compose up --force-recreate`


## Step 7. Create the data view/index pattern
Login to Kibana and Go to `Stack Management` -> `Kibana` -> `Data View` -> `Create`

![Alt text](images/create_data_view.png?raw=true)

go to `stack management` from your sidebar

![Alt text](images/create_data_view.png?raw=true)

type `filebeat-*` and select `@timestamp`

click "Create data view"

## Step 6. Viola! Let us check the docker logs
You should see the following in `http://localhost:5601/app/discover` 
![Alt text](images/docker_logs.png?raw=true)


## Homework
Filebeat has been evolved and can replace logstash to some extent.
![Alt text](images/evo.png?raw=true)

Could you follow the guide and try this different setup? 
https://medium.com/@bcoste/powerful-logging-with-docker-filebeat-and-elasticsearch-8ad021aecd87

Once you have done the above practice, you should be able to set up logging for EasyCRM. Try it out and see how you go. 