# Integrate the ELK with filebeat

![Alt text](images/high_level.png?raw=true)
reference: https://www.freecodecamp.org/news/docker-container-log-analysis-with-elastic-stack-53d5ec9e5953/

## Step 1. Let us fix something in docker-elk repo
Go to the docker-elk folder that you cloned (mentioned in 01.elk_hands_on.md)
```
cd docker-elk
```
Open logstash -> pipeline -> logstash.conf and add 
```
index => "%{[@metadata][beat]}-%{[@metadata][version]}"
```
on line 18 
```
...
output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "elastic"
		password => "changeme"
		index => "%{[@metadata][beat]}-%{[@metadata][version]}"
	}
}
```
This is to index the logs that are sent from filebeats
## Step 2. Rebuild and run the docker-compose
```
docker-compose build
docker-compose up
```
In another window, type 
```
docker-compose ps
```
You should see
```
           Name                         Command               State                                               Ports                                             
--------------------------------------------------------------------------------------------------------------------------------------------------------------------
docker-elk_elasticsearch_1   /bin/tini -- /usr/local/bi ...   Up      0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp                                                
docker-elk_kibana_1          /bin/tini -- /usr/local/bi ...   Up      0.0.0.0:5601->5601/tcp                                                                        
docker-elk_logstash_1        /usr/local/bin/docker-entr ...   Up      0.0.0.0:5000->5000/tcp, 0.0.0.0:5000->5000/udp, 0.0.0.0:5044->5044/tcp, 0.0.0.0:9600->9600/tcp

```



## Step 3. Let us build the filebeat docker container
```
cd filebeat
docker build -t fcc/filebeat .
```
Build the docker image

## Step 4. Run the filebeat agent to collect docker logs
Run filebeat with the following command to collect docker logs 

```
docker run --rm -v '/var/lib/docker/containers:/usr/share/dockerlogs/data:ro' -v '/var/run/docker.sock:/var/run/docker.sock' --name filebeat fcc/filebeat:latest
```

In the above Docker command, note the two bind mount parameters: /var/lib/docker/containers is the path where docker 
logs exist within the host machine, and it has been bound to /usr/share/dockerlogs/data path within Filebeat container
with read only access.

In the second bind mount argument, /var/run/docker.sock is bound into the Filebeat containerâ€™s Docker daemon. 
It is the unix socket the Docker daemon listens on by default and it can be used to communicate with the daemon 
from within a container. This allows our Filebeat container to obtain Docker metadata and enrich the container log 
entries along with the metadata and push it to ELK stack.

## Step 5. login to kibana and try to create the index
Login to Kibana and Go to `Stack Management` -> `Kibana` -> `Data View` -> `Create`

![Alt text](images/create_data_view.png?raw=true)

You will not see `filebeat-7.2.0` index patterns but some generic ones only.

This is because we have errors in our logs

```
logstash_1       | [2022-06-27T11:42:29,498][INFO ][logstash.outputs.elasticsearch][main]
[495aaf9432a30cd7a341bf5aa2516b8e676e8f0b36e9f332d23786beb5574df2] Retrying failed action
 {:status=>403, :action=>["index", {:_id=>nil, :_index=>"filebeat-7.2.0", :routing=>nil}, 
 {"container"=>{"id"=>"3a070ddebde91dece2a008789f27d958bd851b674942efe850c09a1398fe33a2"}, 
 "stream"=>"stderr", "event"=>{"original"=>"W0625 07:31:11.091697       1 watcher.go:229] 
 watch chan error: etcdserver: mvcc: required revision has been compacted"}, 
 "tags"=>["beats_input_codec_plain_applied"], 
 "@version"=>"1", "host"=>{"name"=>"b86a955ef061"}, 
 "input"=>{"type"=>"docker"}, 
 "@timestamp"=>2022-06-25T07:31:11.094Z, 
 "message"=>"W0625 07:31:11.091697       1 watcher.go:229]
  watch chan error: etcdserver: mvcc: required revision has been compacted", 
  "agent"=>{"type"=>"filebeat", "ephemeral_id"=>"0d7f7643-59d2-40fc-b3c5-4174a9af49e9", 
  "hostname"=>"b86a955ef061", "version"=>"7.2.0", "id"=>"387bd947-1740-44fa-aed8-e607f5402b08"}, 
  "ecs"=>{"version"=>"1.0.0"}, "log"=>{"offset"=>168212, 
  "file"=>{"path"=>"/usr/share/dockerlogs/data/3a070ddebde91dece2a008789f27d958bd851b674942efe850c09a1398fe33a2/3a070ddebde91dece2a008789f27d958bd851b674942efe850c09a1398fe33a2-json.log"}}}], 
  :error=>{"type"=>"security_exception", 
                   "reason"=>"action [indices:admin/auto_create] is unauthorized for user [logstash_internal] with roles
                              [logstash_writer] on indices [filebeat-7.2.0], this action is granted by the index 
                              privileges [auto_configure,create_index,manage,all]"}}
```
Search `is unauthorized for user [logstash_internal] with roles
[logstash_writer]`

You will find this thread https://github.com/deviantony/docker-elk/issues/687

## Step 6. Fix the security issue

For simplicity, some may take the option to give logstash_internal a superuser privilege

![Alt text](images/superuser.png?raw=true)

After updating, you should see no more errors. However, you should never do this in production.

Instead, search the error with ChatGPT, you will get a more acceptable answer

The error message you provided appears to be related to a security exception in Elasticsearch. Let me break down the error message for you:

1. `type`: This field indicates the type of exception that occurred, which is a "security_exception" in this case. Security exceptions typically occur when there are issues related to user permissions and access control.

2. `reason`: The "reason" field provides a description of why the security exception occurred. In this case, it states that the action `[indices:admin/auto_create]` is unauthorized for the user `[logstash_internal]` with effective roles `[logstash_writer]` on the indices `[filebeat-7.2.0]`. It also mentions that the required action is granted by the index privileges `[auto_configure, create_index, manage, all]`.

To resolve this issue, you should consider the following steps:

1. **Review User and Role Permissions**: Ensure that the user `[logstash_internal]` has the appropriate permissions and roles assigned. In this case, it appears that the user has the `logstash_writer` role, but it may not have the necessary index privileges to perform the action `[indices:admin/auto_create]`.

2. **Grant Necessary Index Privileges**: As mentioned in the error message, the required action is granted by the index privileges `[auto_configure, create_index, manage, all]`. You may need to modify the user's role to include these index privileges or create a new role with the required privileges and assign it to the user.

3. **Check Index Settings**: Verify the settings of the index `[filebeat-7.2.0]`. Make sure that it allows the necessary actions, including index creation, according to the privileges defined in the user's role.

4. **Review Configuration**: Check your Logstash configuration to ensure that it is using the correct user and role for interacting with Elasticsearch. Any misconfiguration in Logstash can lead to authorization issues.

5. **Elasticsearch Version Compatibility**: Ensure that the version of Elasticsearch, Logstash, and the Beats (e.g., Filebeat) components are compatible and have no known security-related issues. Sometimes, upgrading or patching components can resolve such issues.

6. **Logging and Monitoring**: Implement proper logging and monitoring to help diagnose and troubleshoot security-related issues in your Elasticsearch cluster. This can provide insights into the root cause of the problem.

7. **Consult Elasticsearch Documentation**: If you continue to face issues, consult the official Elasticsearch documentation or seek assistance from your organization's Elasticsearch administrators or support team.

By addressing these points and ensuring that the user has the necessary permissions and index privileges, you should be able to resolve the security exception in Elasticsearch.

---

All you need to do is to go to roles -> logstash_writer -> index privileges add filebeat-* in the indices




## Step 7. Create the data view/index pattern

go to `stack management` from your sidebar

![Alt text](images/create_data_view.png?raw=true)

type `filebeat-*` and select `@timestamp`

click "Create data view"

## Step 6. Viola! Let us check the docker logs
You should see the following in `http://localhost:5601/app/discover` 
![Alt text](images/docker_logs.png?raw=true)


## Homework
Filebeat has been evolved and can replace logstash to some extent.
![Alt text](images/evo.png?raw=true)

Could you follow the guide and try this different setup? 
https://medium.com/@bcoste/powerful-logging-with-docker-filebeat-and-elasticsearch-8ad021aecd87

Once you have done the above practice, you should be able to set up logging for EasyCRM. Try it out and see how you go. 