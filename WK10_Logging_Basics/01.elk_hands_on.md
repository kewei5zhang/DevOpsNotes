# ELK Hands-on

## Step 1. Clone the repo and read the ELK repo 

Let us get a docker-elk
```
git clone https://github.com/deviantony/docker-elk
```

Quickly read through the repo and identify logstash.conf

## Step 2. Spin up the ELK stack

Now, let us spin up the ELK stack by
```
cd docker-elk
docker compose up setup
docker compose up
```
Expecting to see something like this:
```
...
Creating docker-elk_elasticsearch_1 ... done
Creating docker-elk_kibana_1        ... done
Creating docker-elk_logstash_1      ... done
Attaching to docker-elk_elasticsearch_1, docker-elk_logstash_1, docker-elk_kibana_1
logstash_1       | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
elasticsearch_1  | Created elasticsearch keystore in /usr/share/elasticsearch/config/elasticsearch.keystore
kibana_1         | {"type":"log","@timestamp":"2020-07-20T11:03:35Z","tags":["warning","plugins-discovery"],"pid":7,"message":"Expect plugin \"id\" in camelCase, but found: apm_oss"}
```
The stack is pre-configured with the following privileged bootstrap user:
```
user: elastic
password: changeme
```
## Step 3. Change the password
Execute the following three commands in `docker-elk`

```
docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user elastic
docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user logstash_internal
docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user kibana_system
docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user filebeat_internal
docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user beat_system
```

Sample output will look like this

```
❯ docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user elastic
Password for the [elastic] user successfully reset.
New value: Wb7Yv6niUXEayOtNMCl*
❯ docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user logstash_internal
Password for the [logstash_internal] user successfully reset.
New value: 3cHnndylWE0Dm3eHNx6N
❯ docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user kibana_system
Password for the [kibana_system] user successfully reset.
New value: DzA=zWV8dxvTqsm=MVNS
❯ docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user filebeat_internal
Password for the [filebeat_internal] user successfully reset.
New value: AqNqHlFDJr*SDJN5iT0W
❯ docker compose exec elasticsearch bin/elasticsearch-reset-password --batch --user beats_system
Password for the [beats_system] user successfully reset.
New value: QIKj9czB9joOwkO9cHMO
```

Now, update `.env` in docker-elk

and then restart your service
```
docker-compose down
docker-compose up
```

## Step 4. Check the docker containers
Check the containers and logs, make sure they are running
```
docker ps
CONTAINER ID   IMAGE                      COMMAND                  CREATED         STATUS         PORTS                                                                                            NAMES
7054d9fafeb9   docker-elk_logstash        "/usr/local/bin/dock…"   7 minutes ago   Up 7 minutes   0.0.0.0:5000->5000/tcp, 0.0.0.0:5044->5044/tcp, 0.0.0.0:9600->9600/tcp, 0.0.0.0:5000->5000/udp   docker-elk_logstash_1
d506ff044997   docker-elk_kibana          "/bin/tini -- /usr/l…"   7 minutes ago   Up 7 minutes   0.0.0.0:5601->5601/tcp                                                                           docker-elk_kibana_1
2dba2aed0b46   docker-elk_elasticsearch   "/bin/tini -- /usr/l…"   7 minutes ago   Up 7 minutes   0.0.0.0:9200->9200/tcp, 0.0.0.0:9300->9300/tcp                                                   docker-elk_elasticsearch_1
```
ELK stack - FILEBEAT (Installed on your webapp servers) + Logstash + Elastic Search + Kibana

![Alt text](images/high_level.png?raw=true)

* **Filebeat** is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers,
  Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to 
  Elasticsearch or Logstash for indexing.
  
* **Logstash** is a free and open server-side data processing pipeline that ingests data from a multitude of sources, 
  transforms it, and then sends it to your favorite "stash." 
  
* **Elasticsearch** is a distributed, free and open search and analytics engine for all types of data, including textual, 
  numerical, geospatial, structured, and unstructured. Elasticsearch is built on Apache Lucene and was first released 
  in 2010 by Elasticsearch N.V. (now known as Elastic). Known for its simple REST APIs, distributed nature, speed, and
  scalability, Elasticsearch is the central component of the Elastic Stack, a set of free and open tools for data 
  ingestion, enrichment, storage, analysis, and visualization.
  
* **Kibana** is a free and open frontend application that sits on top of the Elastic Stack, providing search and data 
  visualization capabilities for data indexed in Elasticsearch. Commonly known as the charting tool for the Elastic
  Stack (previously referred to as the ELK Stack after Elasticsearch, Logstash, and Kibana), Kibana also acts as the
  user interface for monitoring, managing, and securing an Elastic Stack cluster — as well as the centralized hub for built-in solutions developed on the Elastic Stack.

## Step 5. Check Elastic Search info

Open browser and check. You will need to input username and password generated above.
```
http://localhost:9200
```

## Step 6. Load the test/sample data
Login to localhost:5601 -> Home (http://localhost:5601/app/kibana#/home) -> Try sample data -> Other sample data sets -> Sample web logs 
![Alt text](images/add_sample_data.png?raw=true)

Similarly, if you would like to load your own data via the UI, I would recommend to get a syslog data or generate your
own application log (See WK4 How we generate logfile in python)

## Step 7. Load data from logstash
Update logstash.conf under docker-elk/logstash/pipeline/logstash.conf to add a index for data from logstash

```
output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "logstash_internal"
		password => "${LOGSTASH_INTERNAL_PASSWORD}"
		index => "logstash-%{+YYYY.MM.dd}"
	}
}
```

You can go ahead and inject some log entries. The shipped Logstash configuration allows you to send content via TCP:

```
# Using GNU netcat (CentOS, Fedora, MacOS Homebrew, ...)
$ nc -c localhost 50000 < /var/log/system.log
```

## Step 8. Create data view for logstash-*

Search and navigate to Kabana Data view.

![Alt text](images/logstash-data-view.png?raw=true)



What is `nc`? https://linux.die.net/man/1/nc


Open another terminal and Create an data view (previously known as 'index pattern') via the Kibana API:
```
curl -X POST "http://localhost:5601/api/data_views/data_view" \
-H 'kbn-xsrf: true' \
-H 'Content-Type: application/json' \
-u 'elastic:<your generated elastic password>' \
-d '{
  "data_view": {
     "title": "logstash-*",
     "name": "My Logstash Data View"
  }
}'
```

or you can login to the UI -> "Connect to your Elasticsearch index" -> give index a prefix "logstash-*" and select @timestamp

## Step 9 View the data
Click http://localhost:5601/app/kibana#/discover
![Alt text](images/discover_the_data.png?raw=true)

and you should see sample logs or logstash indexes

Please click through any button on the page to get familiar with it.

## KQL Knowledge
Filter

```
request: /kibana
```

This query filters the data to only show records where the request field exactly matches /kibana. It's a straightforward way to narrow down logs or data entries to those related to a specific page or endpoint.

```
response: 5*
```
This query uses a wildcard (*) to filter records where the response field starts with 5, indicating server error status codes (like 500, 502, 503, etc.). Wildcards expand the search to match any characters in place of the asterisk, making it useful for capturing a range of values.

```
host: *elastic* and not response 200
```

Here, two conditions are combined:

The host field must contain the term elastic, indicated by *elastic* (the asterisks mean the term can appear anywhere within the host field).
The record's response must not be 200.
The and operator is used to require both conditions to be true, while not inversely filters the response field to exclude 200 status codes. This query might be used to find non-successful requests to a server or service with "elastic" in its hostname.

```
referer : http\://facebook.com* 
```
This query filters for records where the referer field starts with http://facebook.com, indicating the traffic came from Facebook. The backslash (\) is used to escape the colon (:), a special character in KQL, ensuring the query interprets it literally as part of the URL, rather than a syntax element.

You must escape `\`
```
\():<>"*
```
In KQL, certain characters have special meanings or functions. If you need to search for these characters literally (for instance, in a query string or a piece of code), you must precede them with a backslash (\) to "escape" them. This tells KQL to treat the following character as part of the search string, rather than interpreting it as part of the query syntax. The characters that need to be escaped include backslashes (\), parentheses ((, )), colons (:), less than/greater than signs (<, >), double quotes ("), and asterisks (*).


## Step 10 Analyse the data
Let us go to dashboards http://localhost:5601/app/kibana#/dashboard
-> Create New -> Lens

What is Lens?
Kibana Lens is an easy-to-use, intuitive UI that simplifies the process of data visualization through a drag-and-drop 
experience. Whether you're exploring billions of logs or spotting trends from your website traffic, Lens gets you from
data to insights in just a few clicks — no prior experience in Kibana required.


Task: What we would like to understand is the health of the webapp: how many requests were returning 2** vs 5**

![Alt text](images/lens.png?raw=true)

Could you also answer the following questions with line, bar or pie charts:
* where do most of the requests coming from over the last 7 days? Location wise and Ip wise?
* what is the percentage of the error logs?
* what are the top requests?
* what extension is used the most?
* what browser do most customers use?


## Homework
1. We would always wanna be notified if the system goes wrong, what does an alert mean?

   * The configuration is not set for this system. Could you do your own research and see how to set up an alert for the logging system?
e.g. alert when the number of 5xx > 10

2. Is it possible to set up dashboards with terraform?